{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4d7e707a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-99-10124c85c35e>, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-99-10124c85c35e>\"\u001b[1;36m, line \u001b[1;32m35\u001b[0m\n\u001b[1;33m    if use_epochsm\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# %load \"C:\\Users\\datho\\OneDrive\\Desktop\\UCL\\Year 1\\Events and Projects\\MAPS research\\Self organising map\\Reports\\minisom-master\\minisom-master\\minisom.py\"\n",
    "from numpy import (array, unravel_index,unravel_multi_index, nditer, linalg, random, subtract, max,\n",
    "                   power, exp, zeros, ones, arange, outer, meshgrid, dot,\n",
    "                   logical_and, mean, cov, argsort, linspace, transpose,\n",
    "                   einsum, prod, nan, sqrt, hstack, diff, argmin, multiply,\n",
    "                   nanmean, nansum, tile, array_equal)\n",
    "from numpy.linalg import norm\n",
    "from collections import defaultdict, Counter\n",
    "from warnings import warn\n",
    "from sys import stdout\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# for unit tests\n",
    "from numpy.testing import assert_almost_equal, assert_array_almost_equal\n",
    "from numpy.testing import assert_array_equal\n",
    "import unittest\n",
    "\n",
    "\"\"\"\n",
    "    Minimalistic implementation of the Self Organizing Maps (SOM).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _build_iteration_indexes(data_len, num_iterations,\n",
    "                             verbose=False, random_generator=None,\n",
    "                             use_epochs=False):\n",
    "    \"\"\"Returns an iterable with the indexes of the samples\n",
    "    to pick at each iteration of the training.\n",
    "\n",
    "    If random_generator is not None, it must be an instance\n",
    "    of numpy.random.RandomState and it will be used\n",
    "    to randomize the order of the samples.\"\"\"\n",
    "    if use_epochsm\n",
    "        iterations_per_epoch = arange(data_len)\n",
    "        if random_generator:\n",
    "            random_generator.shuffle(iterations_per_epoch)\n",
    "        iterations = tile(iterations_per_epoch, num_iterations)\n",
    "    else:\n",
    "        iterations = arange(num_iterations) % data_len\n",
    "        if random_generator:\n",
    "            random_generator.shuffle(iterations)\n",
    "    if verbose:\n",
    "        return _wrap_index__in_verbose(iterations)\n",
    "    else:\n",
    "        return iterations\n",
    "\n",
    "\n",
    "def _wrap_index__in_verbose(iterations):\n",
    "    \"\"\"Yields the values in iterations printing the status on the stdout.\"\"\"\n",
    "    m = len(iterations)\n",
    "    digits = len(str(m))\n",
    "    progress = '\\r [ {s:{d}} / {m} ] {s:3.0f}% - ? it/s'\n",
    "    progress = progress.format(m=m, d=digits, s=0)\n",
    "    stdout.write(progress)\n",
    "    beginning = time()\n",
    "    stdout.write(progress)\n",
    "    for i, it in enumerate(iterations):\n",
    "        yield it\n",
    "        sec_left = ((m-i+1) * (time() - beginning)) / (i+1)\n",
    "        time_left = str(timedelta(seconds=sec_left))[:7]\n",
    "        progress = '\\r [ {i:{d}} / {m} ]'.format(i=i+1, d=digits, m=m)\n",
    "        progress += ' {p:3.0f}%'.format(p=100*(i+1)/m)\n",
    "        progress += ' - {time_left} left '.format(time_left=time_left)\n",
    "        stdout.write(progress)\n",
    "\n",
    "\n",
    "def fast_norm(x):\n",
    "    \"\"\"Returns norm-2 of a 1-D numpy array.\n",
    "\n",
    "    * faster than linalg.norm in case of 1-D arrays (numpy 1.9.2rc1).\n",
    "    \"\"\"\n",
    "    return sqrt(dot(x, x.T))\n",
    "\n",
    "\n",
    "def asymptotic_decay(learning_rate, t, max_iter):\n",
    "    \"\"\"Decay function of the learning process.\n",
    "    Parameters\n",
    "    ----------\n",
    "    learning_rate : float\n",
    "        current learning rate.\n",
    "\n",
    "    t : int\n",
    "        current iteration.\n",
    "\n",
    "    max_iter : int\n",
    "        maximum number of iterations for the training.\n",
    "    \"\"\"\n",
    "    return learning_rate / (1+t/(max_iter/2))\n",
    "\n",
    "\n",
    "class MiniSom(object):\n",
    "    def __init__(self, data, x, y, input_len, num_len sigma=1.0, learning_rate=0.5,\n",
    "                 decay_function=asymptotic_decay,\n",
    "                 neighborhood_function='gaussian', topology='rectangular',\n",
    "                 activation_distance='euclidean', random_seed=None):\n",
    "        \"\"\"Initializes a Self Organizing Maps.\n",
    "\n",
    "        A rule of thumb to set the size of the grid for a dimensionality\n",
    "        reduction task is that it should contain 5*sqrt(N) neurons\n",
    "        where N is the number of samples in the dataset to analyze.\n",
    "\n",
    "        E.g. if your dataset has 150 samples, 5*sqrt(150) = 61.23\n",
    "        hence a map 8-by-8 should perform well.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : int\n",
    "            x dimension of the SOM.\n",
    "\n",
    "        y : int\n",
    "            y dimension of the SOM.\n",
    "\n",
    "        input_len : int\n",
    "            Number of the elements of the vectors in input.\n",
    "        \n",
    "        num_len : int\n",
    "            Number of numerical elements of the vectors in input.\n",
    "\n",
    "        sigma : float, optional (default=1.0)\n",
    "            Spread of the neighborhood function, needs to be adequate\n",
    "            to the dimensions of the map.\n",
    "            (at the iteration t we have sigma(t) = sigma / (1 + t/T)\n",
    "            where T is #num_iteration/2)\n",
    "        learning_rate : initial learning rate\n",
    "            (at the iteration t we have\n",
    "            learning_rate(t) = learning_rate / (1 + t/T)\n",
    "            where T is #num_iteration/2)\n",
    "\n",
    "        decay_function : function (default=None)\n",
    "            Function that reduces learning_rate and sigma at each iteration\n",
    "            the default function is:\n",
    "                        learning_rate / (1+t/(max_iterarations/2))\n",
    "\n",
    "            A custom decay function will need to to take in input\n",
    "            three parameters in the following order:\n",
    "\n",
    "            1. learning rate\n",
    "            2. current iteration\n",
    "            3. maximum number of iterations allowed\n",
    "\n",
    "\n",
    "            Note that if a lambda function is used to define the decay\n",
    "            MiniSom will not be pickable anymore.\n",
    "\n",
    "        neighborhood_function : string, optional (default='gaussian')\n",
    "            Function that weights the neighborhood of a position in the map.\n",
    "            Possible values: 'gaussian', 'mexican_hat', 'bubble', 'triangle'\n",
    "\n",
    "        topology : string, optional (default='rectangular')\n",
    "            Topology of the map.\n",
    "            Possible values: 'rectangular', 'hexagonal'\n",
    "\n",
    "        activation_distance : string, callable optional (default='euclidean')\n",
    "            Distance used to activate the map.\n",
    "            Possible values: 'euclidean', 'cosine', 'manhattan', 'chebyshev'\n",
    "\n",
    "            Example of callable that can be passed:\n",
    "\n",
    "            def euclidean(x, w):\n",
    "                return linalg.norm(subtract(x, w), axis=-1)\n",
    "\n",
    "        random_seed : int, optional (default=None)\n",
    "            Random seed to use.\n",
    "        \"\"\"\n",
    "        if sigma >= x or sigma >= y:\n",
    "            warn('Warning: sigma is too high for the dimension of the map.')\n",
    "\n",
    "        self._random_generator = random.RandomState(random_seed)\n",
    "\n",
    "        self._learning_rate = learning_rate\n",
    "        self._sigma = sigma\n",
    "        self._input_len = input_len\n",
    "        # random initialization\n",
    "        # for numerical variables\n",
    "        self._weights  = self._random_generator.rand(x, y, num_len)*2-1 #change to num later\n",
    "        \n",
    "        #self._weights /= linalg.norm(self._weights, axis=-1, keepdims=True)\n",
    "\n",
    "        #for categorical variables\n",
    "        '''\n",
    "        Here \n",
    "        '''\n",
    "        \n",
    "#         for var in data[]\n",
    "      \n",
    "#         self._cat_randin = random.randint(0,len(var)-1,x*y)\n",
    "#         self.weights_cat.append(self._cat_randin[_cat_randin])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self._activation_map = zeros((x, y))\n",
    "        self._neigx = arange(x)\n",
    "        self._neigy = arange(y)  # used to evaluate the neighborhood function\n",
    "\n",
    "        if topology not in ['hexagonal', 'rectangular']:\n",
    "            msg = '%s not supported only hexagonal and rectangular available'\n",
    "            raise ValueError(msg % topology)\n",
    "        self.topology = topology\n",
    "        self._xx, self._yy = meshgrid(self._neigx, self._neigy)\n",
    "        self._xx = self._xx.astype(float)\n",
    "        self._yy = self._yy.astype(float)\n",
    "        if topology == 'hexagonal':\n",
    "            self._xx[::-2] -= 0.5\n",
    "            if neighborhood_function in ['triangle']:\n",
    "                warn('triangle neighborhood function does not ' +\n",
    "                     'take in account hexagonal topology')\n",
    "\n",
    "        self._decay_function = decay_function\n",
    "\n",
    "        neig_functions = {'gaussian': self._gaussian,\n",
    "                          'mexican_hat': self._mexican_hat,\n",
    "                          'bubble': self._bubble,\n",
    "                          'triangle': self._triangle}\n",
    "\n",
    "        if neighborhood_function not in neig_functions:\n",
    "            msg = '%s not supported. Functions available: %s'\n",
    "            raise ValueError(msg % (neighborhood_function,\n",
    "                                    ', '.join(neig_functions.keys())))\n",
    "\n",
    "        if neighborhood_function in ['triangle',\n",
    "                                     'bubble'] and (divmod(sigma, 1)[1] != 0\n",
    "                                                    or sigma < 1):\n",
    "            warn('sigma should be an integer >=1 when triangle or bubble' +\n",
    "                 'are used as neighborhood function')\n",
    "\n",
    "        self.neighborhood = neig_functions[neighborhood_function]\n",
    "\n",
    "        distance_functions = {'euclidean': self._euclidean_distance,\n",
    "                              'cosine': self._cosine_distance,\n",
    "                              'manhattan': self._manhattan_distance,\n",
    "                              'chebyshev': self._chebyshev_distance}\n",
    "\n",
    "        if isinstance(activation_distance, str):\n",
    "            if activation_distance not in distance_functions:\n",
    "                msg = '%s not supported. Distances available: %s'\n",
    "                raise ValueError(msg % (activation_distance,\n",
    "                                        ', '.join(distance_functions.keys())))\n",
    "\n",
    "            self._activation_distance = distance_functions[activation_distance]\n",
    "        elif callable(activation_distance):\n",
    "            self._activation_distance = activation_distance\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"Returns the weights of the neural network.\"\"\"\n",
    "        return self._weights\n",
    "\n",
    "    def get_euclidean_coordinates(self):\n",
    "        \"\"\"Returns the position of the neurons on an euclidean\n",
    "        plane that reflects the chosen topology in two meshgrids xx and yy.\n",
    "        Neuron with map coordinates (1, 4) has coordinate (xx[1, 4], yy[1, 4])\n",
    "        in the euclidean plane.\n",
    "\n",
    "        Only useful if the topology chosen is not rectangular.\n",
    "        \"\"\"\n",
    "        return self._xx.T, self._yy.T\n",
    "\n",
    "    def convert_map_to_euclidean(self, xy):\n",
    "        \"\"\"Converts map coordinates into euclidean coordinates\n",
    "        that reflects the chosen topology.\n",
    "\n",
    "        Only useful if the topology chosen is not rectangular.\n",
    "        \"\"\"\n",
    "        return self._xx.T[xy], self._yy.T[xy]\n",
    "\n",
    "    def _activate(self, x):\n",
    "        \"\"\"Updates matrix activation_map, in this matrix\n",
    "           the element i,j is the response of the neuron i,j to x.\"\"\"\n",
    "        self._activation_map = self._activation_distance(x, self._weights)\n",
    "\n",
    "    def activate(self, x):\n",
    "        \"\"\"Returns the activation map to x.\"\"\"\n",
    "        self._activate(x)\n",
    "        return self._activation_map\n",
    "\n",
    "    def _gaussian(self, c, sigma):\n",
    "        \"\"\"Returns a Gaussian centered in c.\"\"\"\n",
    "        d = 2*sigma*sigma\n",
    "        ax = exp(-power(self._xx-self._xx.T[c], 2)/d)\n",
    "        ay = exp(-power(self._yy-self._yy.T[c], 2)/d)\n",
    "        return (ax * ay).T  # the external product gives a matrix\n",
    "\n",
    "    def _mexican_hat(self, c, sigma):\n",
    "        \"\"\"Mexican hat centered in c.\"\"\"\n",
    "        p = power(self._xx-self._xx.T[c], 2) + power(self._yy-self._yy.T[c], 2)\n",
    "        d = 2*sigma*sigma\n",
    "        return (exp(-p/d)*(1-2/d*p)).T\n",
    "\n",
    "    def _bubble(self, c, sigma):\n",
    "        \"\"\"Constant function centered in c with spread sigma.\n",
    "        sigma should be an odd value.\n",
    "        \"\"\"\n",
    "        ax = logical_and(self._neigx > c[0]-sigma,\n",
    "                         self._neigx < c[0]+sigma)\n",
    "        ay = logical_and(self._neigy > c[1]-sigma,\n",
    "                         self._neigy < c[1]+sigma)\n",
    "        return outer(ax, ay)*1.\n",
    "\n",
    "    def _triangle(self, c, sigma):\n",
    "        \"\"\"Triangular function centered in c with spread sigma.\"\"\"\n",
    "        triangle_x = (-abs(c[0] - self._neigx)) + sigma\n",
    "        triangle_y = (-abs(c[1] - self._neigy)) + sigma\n",
    "        triangle_x[triangle_x < 0] = 0.\n",
    "        triangle_y[triangle_y < 0] = 0.\n",
    "        return outer(triangle_x, triangle_y)\n",
    "\n",
    "    def _cosine_distance(self, x, w):\n",
    "        num = (w * x).sum(axis=2)\n",
    "        denum = multiply(linalg.norm(w, axis=2), linalg.norm(x))\n",
    "        return 1 - num / (denum+1e-8)\n",
    "\n",
    "    def _euclidean_distance(self, x, w):\n",
    "        return linalg.norm(subtract(x, w), axis=-1)\n",
    "\n",
    "    def _manhattan_distance(self, x, w):\n",
    "        return linalg.norm(subtract(x, w), ord=1, axis=-1)\n",
    "\n",
    "    def _chebyshev_distance(self, x, w):\n",
    "        return max(subtract(x, w), axis=-1)\n",
    "\n",
    "    def _check_iteration_number(self, num_iteration):\n",
    "        if num_iteration < 1:\n",
    "            raise ValueError('num_iteration must be > 1')\n",
    "\n",
    "    def _check_input_len(self, data):\n",
    "        \"\"\"Checks that the data in input is of the correct shape.\"\"\"\n",
    "        data_len = len(data[0])\n",
    "        if self._input_len != data_len:\n",
    "            msg = 'Received %d features, expected %d.' % (data_len,\n",
    "                                                          self._input_len)\n",
    "            raise ValueError(msg)\n",
    "\n",
    "    def winner(self, x):\n",
    "        \"\"\"Computes the coordinates of the winning neuron for the sample x.\"\"\"\n",
    "        '''the original winner() version does not work on arrays as they only calculate one for each iteration.\n",
    "        So for the batch algorithm, it will be modified to return a list of winning neurons\n",
    "        '''\n",
    "        if len(x) == 1:\n",
    "            self._activate(x)\n",
    "            return unravel_index(self._activation_map.argmin(),\n",
    "                             self._activation_map.shape)\n",
    "        if len(x) > 1:\n",
    "            winner_chunk = []\n",
    "            for i in x:\n",
    "                self._activate(i)\n",
    "                winnie = unravel_index(self._activation_map.argmin(),\n",
    "                                 self._activation_map.shape)\n",
    "                winner_chunk.append(winnie)\n",
    "            return winner_chunk\n",
    "\n",
    "    def update(self, x, win, t, max_iteration):\n",
    "        \"\"\"Updates the weights of the neurons.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            Current pattern to learn.\n",
    "        win : tuple\n",
    "            Position of the winning neuron for x (array or tuple).\n",
    "        t : int\n",
    "            rate of decay for sigma and learning rate\n",
    "        max_iteration : int\n",
    "            If use_epochs is True:\n",
    "                Number of epochs the SOM will be trained for\n",
    "            If use_epochs is False:\n",
    "                Maximum number of iterations (one iteration per sample).\n",
    "        \"\"\"\n",
    "        eta = self._decay_function(self._learning_rate, t, max_iteration)\n",
    "        # sigma and learning rate decrease with the same rule\n",
    "        sig = self._decay_function(self._sigma, t, max_iteration)\n",
    "       \n",
    "        #creating a list of h, reflecting the position of the current neuron with all the winning neurons\n",
    "        h_list = []\n",
    "        for i in win:\n",
    "            h = self.neighborhood(i, sig)*eta\n",
    "            h_list.append(h)\n",
    "        \n",
    "        # w_new = sum(trans(h)*x)/sum(h))\n",
    "        numerator = np.dot(x,h_list)\n",
    "        denominator = sum(h_list)\n",
    "    \n",
    "        self._weights = numerator/denominator\n",
    "\n",
    "    def quantization(self, data):\n",
    "        \"\"\"Assigns a code book (weights vector of the winning neuron)\n",
    "        to each sample in data.\"\"\"\n",
    "        self._check_input_len(data)\n",
    "        winners_coords = argmin(self._distance_from_weights(data), axis=1)\n",
    "        return self._weights[unravel_index(winners_coords,\n",
    "                                           self._weights.shape[:2])]\n",
    "\n",
    "    def random_weights_init(self, data):\n",
    "        \"\"\"Initializes the weights of the SOM\n",
    "        picking random samples from data.\"\"\"\n",
    "        self._check_input_len(data)\n",
    "        it = nditer(self._activation_map, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            rand_i = self._random_generator.randint(len(data))\n",
    "            self._weights[it.multi_index] = data[rand_i]\n",
    "            it.iternext()\n",
    "\n",
    "    def pca_weights_init(self, data):\n",
    "        \"\"\"Initializes the weights to span the first two principal components.\n",
    "\n",
    "        This initialization doesn't depend on random processes and\n",
    "        makes the training process converge faster.\n",
    "\n",
    "        It is strongly reccomended to normalize the data before initializing\n",
    "        the weights and use the same normalization for the training data.\n",
    "        \"\"\"\n",
    "        if self._input_len == 1:\n",
    "            msg = 'The data needs at least 2 features for pca initialization'\n",
    "            raise ValueError(msg)\n",
    "        self._check_input_len(data)\n",
    "        if len(self._neigx) == 1 or len(self._neigy) == 1:\n",
    "            msg = 'PCA initialization inappropriate:' + \\\n",
    "                  'One of the dimensions of the map is 1.'\n",
    "            warn(msg)\n",
    "        pc_length, pc = linalg.eig(cov(transpose(data)))\n",
    "        pc_order = argsort(-pc_length)\n",
    "        for i, c1 in enumerate(linspace(-1, 1, len(self._neigx))):\n",
    "            for j, c2 in enumerate(linspace(-1, 1, len(self._neigy))):\n",
    "                self._weights[i, j] = c1*pc[pc_order[0]] + c2*pc[pc_order[1]]\n",
    "\n",
    "    def train(self, data, num_iteration,\n",
    "              random_order=False, verbose=False, use_epochs=False):\n",
    "        \"\"\"Trains the SOM.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.array or list\n",
    "            Data matrix.\n",
    "\n",
    "        num_iteration : int\n",
    "            If use_epochs is False, the weights will be\n",
    "            updated num_iteration times. Otherwise they will be updated\n",
    "            len(data)*num_iteration times.\n",
    "\n",
    "        random_order : bool (default=False)\n",
    "            If True, samples are picked in random order.\n",
    "            Otherwise the samples are picked sequentially.\n",
    "\n",
    "        verbose : bool (default=False)\n",
    "            If True the status of the training will be\n",
    "            printed each time the weights are updated.\n",
    "\n",
    "        use_epochs : bool (default=False)\n",
    "            If True the SOM will be trained for num_iteration epochs.\n",
    "            In one epoch the weights are updated len(data) times and\n",
    "            the learning rate is constat throughout a single epoch.\n",
    "        \"\"\"\n",
    "        self._check_iteration_number(num_iteration)\n",
    "        self._check_input_len(data)\n",
    "        random_generator = None\n",
    "        if random_order:\n",
    "            random_generator = self._random_generator\n",
    "        iterations = _build_iteration_indexes(len(data), num_iteration,\n",
    "                                              verbose, random_generator,\n",
    "                                              use_epochs)\n",
    "        if use_epochs:\n",
    "            def get_decay_rate(iteration_index, data_len):\n",
    "                return int(iteration_index / data_len)\n",
    "        else:\n",
    "            def get_decay_rate(iteration_index, data_len):\n",
    "                return int(iteration_index)\n",
    "        for t, iteration in enumerate(iterations):\n",
    "            decay_rate = get_decay_rate(t, len(data))\n",
    "            self.update(data[iteration], self.winner(data[iteration]),\n",
    "                        decay_rate, num_iteration)\n",
    "        if verbose:\n",
    "            print('\\n quantization error:', self.quantization_error(data))\n",
    "\n",
    "    def train_random(self, data, num_iteration, verbose=False):\n",
    "        \"\"\"Trains the SOM picking samples at random from data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.array or list\n",
    "            Data matrix.\n",
    "\n",
    "        num_iteration : int\n",
    "            Maximum number of iterations (one iteration per sample).\n",
    "\n",
    "        verbose : bool (default=False)\n",
    "            If True the status of the training\n",
    "            will be printed at each time the weights are updated.\n",
    "        \"\"\"\n",
    "        self.train(data, num_iteration, random_order=True, verbose=verbose)\n",
    "\n",
    "    def train_batch(self, data, num_iteration, verbose=False):\n",
    "        \"\"\"Trains the SOM using all the vectors in data sequentially.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.array or list\n",
    "            Data matrix.\n",
    "\n",
    "        num_iteration : int\n",
    "            Maximum number of iterations (one iteration per sample).\n",
    "\n",
    "        verbose : bool (default=False)\n",
    "            If True the status of the training\n",
    "            will be printed at each time the weights are updated.\n",
    "        \"\"\"\n",
    "        self.train(data, num_iteration, random_order=False, verbose=verbose)\n",
    "\n",
    "    def distance_map(self, scaling='sum'):\n",
    "        \"\"\"Returns the distance map of the weights.\n",
    "        If scaling is 'sum' (default), each cell is the normalised sum of\n",
    "        the distances between a neuron and its neighbours. Note that this\n",
    "        method uses the euclidean distance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        scaling : string (default='sum')\n",
    "            If set to 'mean', each cell will be the normalized\n",
    "            by the average of the distances of the neighbours.\n",
    "            If set to 'sum', the normalization is done\n",
    "            by the sum of the distances.\n",
    "        \"\"\"\n",
    "\n",
    "        if scaling not in ['sum', 'mean']:\n",
    "            raise ValueError(f'scaling should be either \"sum\" or \"mean\" ('\n",
    "                             f'\"{scaling}\" not valid)')\n",
    "\n",
    "        um = nan * zeros((self._weights.shape[0],\n",
    "                          self._weights.shape[1],\n",
    "                          8))  # 2 spots more for hexagonal topology\n",
    "\n",
    "        ii = [[0, -1, -1, -1, 0, 1, 1, 1]]*2\n",
    "        jj = [[-1, -1, 0, 1, 1, 1, 0, -1]]*2\n",
    "\n",
    "        if self.topology == 'hexagonal':\n",
    "            ii = [[1, 1, 1, 0, -1, 0], [0, 1, 0, -1, -1, -1]]\n",
    "            jj = [[1, 0, -1, -1, 0, 1], [1, 0, -1, -1, 0, 1]]\n",
    "\n",
    "        for x in range(self._weights.shape[0]):\n",
    "            for y in range(self._weights.shape[1]):\n",
    "                w_2 = self._weights[x, y]\n",
    "                e = y % 2 == 0   # only used on hexagonal topology\n",
    "                for k, (i, j) in enumerate(zip(ii[e], jj[e])):\n",
    "                    if (x+i >= 0 and x+i < self._weights.shape[0] and\n",
    "                            y+j >= 0 and y+j < self._weights.shape[1]):\n",
    "                        w_1 = self._weights[x+i, y+j]\n",
    "                        um[x, y, k] = fast_norm(w_2-w_1)\n",
    "\n",
    "        if scaling == 'mean':\n",
    "            um = nanmean(um, axis=2)\n",
    "        if scaling == 'sum':\n",
    "            um = nansum(um, axis=2)\n",
    "\n",
    "        return um/um.max()\n",
    "\n",
    "    def activation_response(self, data):\n",
    "        \"\"\"\n",
    "            Returns a matrix where the element i,j is the number of times\n",
    "            that the neuron i,j have been winner.\n",
    "        \"\"\"\n",
    "        self._check_input_len(data)\n",
    "        a = zeros((self._weights.shape[0], self._weights.shape[1]))\n",
    "        for x in data:\n",
    "            a[self.winner(x)] += 1\n",
    "        return a\n",
    "\n",
    "    def _distance_from_weights(self, data):\n",
    "        \"\"\"Returns a matrix d where d[i,j] is the euclidean distance between\n",
    "        data[i] and the j-th weight.\n",
    "        \"\"\"\n",
    "        input_data = array(data)\n",
    "        weights_flat = self._weights.reshape(-1, self._weights.shape[2])\n",
    "        input_data_sq = power(input_data, 2).sum(axis=1, keepdims=True)\n",
    "        weights_flat_sq = power(weights_flat, 2).sum(axis=1, keepdims=True)\n",
    "        cross_term = dot(input_data, weights_flat.T)\n",
    "        return sqrt(-2 * cross_term + input_data_sq + weights_flat_sq.T)\n",
    "\n",
    "    def quantization_error(self, data):\n",
    "        \"\"\"Returns the quantization error computed as the average\n",
    "        distance between each input sample and its best matching unit.\"\"\"\n",
    "        self._check_input_len(data)\n",
    "        return norm(data-self.quantization(data), axis=1).mean()\n",
    "\n",
    "    def topographic_error(self, data):\n",
    "        \"\"\"Returns the topographic error computed by finding\n",
    "        the best-matching and second-best-matching neuron in the map\n",
    "        for each input and then evaluating the positions.\n",
    "\n",
    "        A sample for which these two nodes are not adjacent counts as\n",
    "        an error. The topographic error is given by the\n",
    "        the total number of errors divided by the total of samples.\n",
    "\n",
    "        If the topographic error is 0, no error occurred.\n",
    "        If 1, the topology was not preserved for any of the samples.\"\"\"\n",
    "        self._check_input_len(data)\n",
    "        total_neurons = prod(self._activation_map.shape)\n",
    "        if total_neurons == 1:\n",
    "            warn('The topographic error is not defined for a 1-by-1 map.')\n",
    "            return nan\n",
    "        if self.topology == 'hexagonal':\n",
    "            return self._topographic_error_hexagonal(data)\n",
    "        else:\n",
    "            return self._topographic_error_rectangular(data)\n",
    "\n",
    "    def _topographic_error_hexagonal(self, data):\n",
    "        \"\"\"Return the topographic error for hexagonal grid\"\"\"\n",
    "        b2mu_inds = argsort(self._distance_from_weights(data), axis=1)[:, :2]\n",
    "        b2mu_coords = [[self._get_euclidean_coordinates_from_index(bmu[0]),\n",
    "                        self._get_euclidean_coordinates_from_index(bmu[1])]\n",
    "                       for bmu in b2mu_inds]\n",
    "        b2mu_coords = array(b2mu_coords)\n",
    "        b2mu_neighbors = [(bmu1 >= bmu2-1) & ((bmu1 <= bmu2+1))\n",
    "                          for bmu1, bmu2 in b2mu_coords]\n",
    "        b2mu_neighbors = [neighbors.prod() for neighbors in b2mu_neighbors]\n",
    "        te = 1 - mean(b2mu_neighbors)\n",
    "        return te\n",
    "\n",
    "    def _topographic_error_rectangular(self, data):\n",
    "        \"\"\"Return the topographic error for rectangular grid\"\"\"\n",
    "        t = 1.42\n",
    "        # b2mu: best 2 matching units\n",
    "        b2mu_inds = argsort(self._distance_from_weights(data), axis=1)[:, :2]\n",
    "        b2my_xy = unravel_index(b2mu_inds, self._weights.shape[:2])\n",
    "        b2mu_x, b2mu_y = b2my_xy[0], b2my_xy[1]\n",
    "        dxdy = hstack([diff(b2mu_x), diff(b2mu_y)])\n",
    "        distance = norm(dxdy, axis=1)\n",
    "        return (distance > t).mean()\n",
    "\n",
    "    def _get_euclidean_coordinates_from_index(self, index):\n",
    "        \"\"\"Returns the Euclidean coordinated of a neuron using its\n",
    "        index as the input\"\"\"\n",
    "        if index < 0:\n",
    "            return (-1, -1)\n",
    "        y = self._weights.shape[1]\n",
    "        coords = self.convert_map_to_euclidean((index % y, int(index/y)))\n",
    "        return coords\n",
    "\n",
    "    def win_map(self, data, return_indices=False):\n",
    "        \"\"\"Returns a dictionary wm where wm[(i,j)] is a list with:\n",
    "        - all the patterns that have been mapped to the position (i,j),\n",
    "          if return_indices=False (default)\n",
    "        - all indices of the elements that have been mapped to the\n",
    "          position (i,j) if return_indices=True\"\"\"\n",
    "        self._check_input_len(data)\n",
    "        winmap = defaultdict(list)\n",
    "        for i, x in enumerate(data):\n",
    "            winmap[self.winner(x)].append(i if return_indices else x)\n",
    "        return winmap\n",
    "\n",
    "    def labels_map(self, data, labels):\n",
    "        \"\"\"Returns a dictionary wm where wm[(i,j)] is a dictionary\n",
    "        that contains the number of samples from a given label\n",
    "        that have been mapped in position i,j.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.array or list\n",
    "            Data matrix.\n",
    "\n",
    "        label : np.array or list\n",
    "            Labels for each sample in data.\n",
    "        \"\"\"\n",
    "        self._check_input_len(data)\n",
    "        if not len(data) == len(labels):\n",
    "            raise ValueError('data and labels must have the same length.')\n",
    "        winmap = defaultdict(list)\n",
    "        for x, l in zip(data, labels):\n",
    "            winmap[self.winner(x)].append(l)\n",
    "        for position in winmap:\n",
    "            winmap[position] = Counter(winmap[position])\n",
    "        return winmap\n",
    "\n",
    "\n",
    "class TestMinisom(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.som = MiniSom(5, 5, 1)\n",
    "        for i in range(5):\n",
    "            for j in range(5):\n",
    "                # checking weights normalization\n",
    "                assert_almost_equal(1.0, linalg.norm(self.som._weights[i, j]))\n",
    "        self.som._weights = zeros((5, 5, 1))  # fake weights\n",
    "        self.som._weights[2, 3] = 5.0\n",
    "        self.som._weights[1, 1] = 2.0\n",
    "\n",
    "    def test_decay_function(self):\n",
    "        assert self.som._decay_function(1., 2., 3.) == 1./(1.+2./(3./2))\n",
    "\n",
    "    def test_fast_norm(self):\n",
    "        assert fast_norm(array([1, 3])) == sqrt(1+9)\n",
    "\n",
    "    def test_euclidean_distance(self):\n",
    "        x = zeros((1, 2))\n",
    "        w = ones((2, 2, 2))\n",
    "        d = self.som._euclidean_distance(x, w)\n",
    "        assert_array_almost_equal(d, [[1.41421356, 1.41421356],\n",
    "                                      [1.41421356, 1.41421356]])\n",
    "\n",
    "    def test_cosine_distance(self):\n",
    "        x = zeros((1, 2))\n",
    "        w = ones((2, 2, 2))\n",
    "        d = self.som._cosine_distance(x, w)\n",
    "        assert_array_almost_equal(d, [[1., 1.],\n",
    "                                      [1., 1.]])\n",
    "\n",
    "    def test_manhattan_distance(self):\n",
    "        x = zeros((1, 2))\n",
    "        w = ones((2, 2, 2))\n",
    "        d = self.som._manhattan_distance(x, w)\n",
    "        assert_array_almost_equal(d, [[2., 2.],\n",
    "                                      [2., 2.]])\n",
    "\n",
    "    def test_chebyshev_distance(self):\n",
    "        x = array([1, 3])\n",
    "        w = ones((2, 2, 2))\n",
    "        d = self.som._chebyshev_distance(x, w)\n",
    "        assert_array_almost_equal(d, [[2., 2.],\n",
    "                                      [2., 2.]])\n",
    "\n",
    "    def test_check_input_len(self):\n",
    "        with self.assertRaises(ValueError):\n",
    "            self.som.train_batch([[1, 2]], 1)\n",
    "\n",
    "        with self.assertRaises(ValueError):\n",
    "            self.som.random_weights_init(array([[1, 2]]))\n",
    "\n",
    "        with self.assertRaises(ValueError):\n",
    "            self.som._check_input_len(array([[1, 2]]))\n",
    "\n",
    "        self.som._check_input_len(array([[1]]))\n",
    "        self.som._check_input_len([[1]])\n",
    "\n",
    "    def test_unavailable_neigh_function(self):\n",
    "        with self.assertRaises(ValueError):\n",
    "            MiniSom(5, 5, 1, neighborhood_function='boooom')\n",
    "\n",
    "    def test_unavailable_distance_function(self):\n",
    "        with self.assertRaises(ValueError):\n",
    "            MiniSom(5, 5, 1, activation_distance='ridethewave')\n",
    "\n",
    "    def test_gaussian(self):\n",
    "        bell = self.som._gaussian((2, 2), 1)\n",
    "        assert bell.max() == 1.0\n",
    "        assert bell.argmax() == 12  # unravel(12) = (2,2)\n",
    "\n",
    "    def test_mexican_hat(self):\n",
    "        bell = self.som._mexican_hat((2, 2), 1)\n",
    "        assert bell.max() == 1.0\n",
    "        assert bell.argmax() == 12  # unravel(12) = (2,2)\n",
    "\n",
    "    def test_bubble(self):\n",
    "        bubble = self.som._bubble((2, 2), 1)\n",
    "        assert bubble[2, 2] == 1\n",
    "        assert sum(sum(bubble)) == 1\n",
    "\n",
    "    def test_triangle(self):\n",
    "        bubble = self.som._triangle((2, 2), 1)\n",
    "        assert bubble[2, 2] == 1\n",
    "        assert sum(sum(bubble)) == 1\n",
    "\n",
    "    def test_win_map(self):\n",
    "        winners = self.som.win_map([[5.0], [2.0]])\n",
    "        assert winners[(2, 3)][0] == [5.0]\n",
    "        assert winners[(1, 1)][0] == [2.0]\n",
    "\n",
    "    def test_win_map_indices(self):\n",
    "        winners = self.som.win_map([[5.0], [2.0]], return_indices=True)\n",
    "        assert winners[(2, 3)] == [0]\n",
    "        assert winners[(1, 1)] == [1]\n",
    "\n",
    "    def test_labels_map(self):\n",
    "        labels_map = self.som.labels_map([[5.0], [2.0]], ['a', 'b'])\n",
    "        assert labels_map[(2, 3)]['a'] == 1\n",
    "        assert labels_map[(1, 1)]['b'] == 1\n",
    "        with self.assertRaises(ValueError):\n",
    "            self.som.labels_map([[5.0]], ['a', 'b'])\n",
    "\n",
    "    def test_activation_reponse(self):\n",
    "        response = self.som.activation_response([[5.0], [2.0]])\n",
    "        assert response[2, 3] == 1\n",
    "        assert response[1, 1] == 1\n",
    "\n",
    "    def test_activate(self):\n",
    "        assert self.som.activate(5.0).argmin() == 13.0  # unravel(13) = (2,3)\n",
    "\n",
    "    def test_distance_from_weights(self):\n",
    "        data = arange(-5, 5).reshape(-1, 1)\n",
    "        weights = self.som._weights.reshape(-1, self.som._weights.shape[2])\n",
    "        distances = self.som._distance_from_weights(data)\n",
    "        for i in range(len(data)):\n",
    "            for j in range(len(weights)):\n",
    "                assert(distances[i][j] == norm(data[i] - weights[j]))\n",
    "\n",
    "    def test_quantization_error(self):\n",
    "        assert self.som.quantization_error([[5], [2]]) == 0.0\n",
    "        assert self.som.quantization_error([[4], [1]]) == 1.0\n",
    "\n",
    "    def test_topographic_error(self):\n",
    "        # 5 will have bmu_1 in (2,3) and bmu_2 in (2, 4)\n",
    "        # which are in the same neighborhood\n",
    "        self.som._weights[2, 4] = 6.0\n",
    "        # 15 will have bmu_1 in (4, 4) and bmu_2 in (0, 0)\n",
    "        # which are not in the same neighborhood\n",
    "        self.som._weights[4, 4] = 15.0\n",
    "        self.som._weights[0, 0] = 14.\n",
    "        assert self.som.topographic_error([[5]]) == 0.0\n",
    "        assert self.som.topographic_error([[15]]) == 1.0\n",
    "\n",
    "        self.som.topology = 'hexagonal'\n",
    "        # 10 will have bmu_1 in (0, 4) and bmu_2 in (1, 3)\n",
    "        # which are in the same neighborhood on a hexagonal grid\n",
    "        self.som._weights[0, 4] = 10.0\n",
    "        self.som._weights[1, 3] = 9.0\n",
    "        # 3 will have bmu_1 in (2, 0) and bmu_2 in (1, 1)\n",
    "        # which are in the same neighborhood on a hexagonal grid\n",
    "        self.som._weights[2, 0] = 3.0\n",
    "        assert self.som.topographic_error([[10]]) == 0.0\n",
    "        assert self.som.topographic_error([[3]]) == 0.0\n",
    "        # True for both hexagonal and rectangular grids\n",
    "        assert self.som.topographic_error([[5]]) == 0.0\n",
    "        assert self.som.topographic_error([[15]]) == 1.0\n",
    "        self.som.topology = 'rectangular'\n",
    "\n",
    "    def test_quantization(self):\n",
    "        q = self.som.quantization(array([[4], [2]]))\n",
    "        assert q[0] == 5.0\n",
    "        assert q[1] == 2.0\n",
    "\n",
    "    def test_random_seed(self):\n",
    "        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        # same initialization\n",
    "        assert_array_almost_equal(som1._weights, som2._weights)\n",
    "        data = random.rand(100, 2)\n",
    "        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        som1.train_random(data, 10)\n",
    "        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        som2.train_random(data, 10)\n",
    "        # same state after training\n",
    "        assert_array_almost_equal(som1._weights, som2._weights)\n",
    "\n",
    "    def test_train_batch(self):\n",
    "        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        data = array([[4, 2], [3, 1]])\n",
    "        q1 = som.quantization_error(data)\n",
    "        som.train(data, 10)\n",
    "        assert q1 > som.quantization_error(data)\n",
    "\n",
    "        data = array([[1, 5], [6, 7]])\n",
    "        q1 = som.quantization_error(data)\n",
    "        som.train_batch(data, 10, verbose=True)\n",
    "        assert q1 > som.quantization_error(data)\n",
    "\n",
    "    def test_train_random(self):\n",
    "        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        data = array([[4, 2], [3, 1]])\n",
    "        q1 = som.quantization_error(data)\n",
    "        som.train(data, 10, random_order=True)\n",
    "        assert q1 > som.quantization_error(data)\n",
    "\n",
    "        data = array([[1, 5], [6, 7]])\n",
    "        q1 = som.quantization_error(data)\n",
    "        som.train_random(data, 10, verbose=True)\n",
    "        assert q1 > som.quantization_error(data)\n",
    "\n",
    "    def test_train_use_epochs(self):\n",
    "        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        data = array([[4, 2], [3, 1]])\n",
    "        q1 = som.quantization_error(data)\n",
    "        som.train(data, 10, use_epochs=True)\n",
    "        assert q1 > som.quantization_error(data)\n",
    "\n",
    "    def test_use_epochs_variables(self):\n",
    "        len_data = 100000\n",
    "        num_epochs = 100\n",
    "        random_gen = random.RandomState(1)\n",
    "        iterations = _build_iteration_indexes(len_data, num_epochs,\n",
    "                                              random_generator=random_gen,\n",
    "                                              use_epochs=True)\n",
    "        assert num_epochs*len_data == len(iterations)\n",
    "\n",
    "        # checks whether all epochs share the same order of indexes\n",
    "        first_epoch = iterations[0:len_data]\n",
    "        for i in range(num_epochs):\n",
    "            i_epoch = iterations[i*len_data:(i+1)*len_data]\n",
    "            assert array_equal(first_epoch, i_epoch)\n",
    "\n",
    "        # checks whether the decay_factor stays constant during one epoch\n",
    "        # and that its values range from 0 to num_epochs-1\n",
    "        decay_factors = []\n",
    "        for t, iteration in enumerate(iterations):\n",
    "            decay_factor = int(t / len_data)\n",
    "            decay_factors.append(decay_factor)\n",
    "        for i in range(num_epochs):\n",
    "            decay_factors_i_epoch = decay_factors[i*len_data:(i+1)*len_data]\n",
    "            assert decay_factors_i_epoch == [i]*len_data\n",
    "\n",
    "    def test_random_weights_init(self):\n",
    "        som = MiniSom(2, 2, 2, random_seed=1)\n",
    "        som.random_weights_init(array([[1.0, .0]]))\n",
    "        for w in som._weights:\n",
    "            assert_array_equal(w[0], array([1.0, .0]))\n",
    "\n",
    "    def test_pca_weights_init(self):\n",
    "        som = MiniSom(2, 2, 2)\n",
    "        som.pca_weights_init(array([[1.,  0.], [0., 1.], [1., 0.], [0., 1.]]))\n",
    "        expected = array([[[0., -1.41421356], [-1.41421356, 0.]],\n",
    "                          [[1.41421356, 0.], [0., 1.41421356]]])\n",
    "        assert_array_almost_equal(som._weights, expected)\n",
    "\n",
    "    def test_distance_map(self):\n",
    "        som = MiniSom(2, 2, 2, random_seed=1)\n",
    "        som._weights = array([[[1.,  0.], [0., 1.]], [[1., 0.], [0., 1.]]])\n",
    "        assert_array_equal(som.distance_map(), array([[1., 1.], [1., 1.]]))\n",
    "\n",
    "        som = MiniSom(2, 2, 2, topology='hexagonal', random_seed=1)\n",
    "        som._weights = array([[[1.,  0.], [0., 1.]], [[1., 0.], [0., 1.]]])\n",
    "        assert_array_equal(som.distance_map(), array([[.5, 1.], [1., .5]]))\n",
    "\n",
    "        som = MiniSom(3, 3, 1, random_seed=1)\n",
    "        som._weights = array([[1, 0, 1], [0, 1, 0], [1, 0, 1]])\n",
    "        dist = array([[2/3, 3/5, 2/3], [3/5, 4/8, 3/5], [2/3, 3/5, 2/3]])\n",
    "        assert_array_equal(som.distance_map(scaling='mean'), dist/max(dist))\n",
    "\n",
    "        with self.assertRaises(ValueError):\n",
    "            som.distance_map(scaling='puppies')\n",
    "\n",
    "    def test_pickling(self):\n",
    "        with open('som.p', 'wb') as outfile:\n",
    "            pickle.dump(self.som, outfile)\n",
    "        with open('som.p', 'rb') as infile:\n",
    "            pickle.load(infile)\n",
    "        os.remove('som.p')\n",
    "\n",
    "    def test_callable_activation_distance(self):\n",
    "        def euclidean(x, w):\n",
    "            return linalg.norm(subtract(x, w), axis=-1)\n",
    "\n",
    "        data = random.rand(100, 2)\n",
    "        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5,\n",
    "                       activation_distance=euclidean, random_seed=1)\n",
    "        som1.train_random(data, 10)\n",
    "        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n",
    "        som2.train_random(data, 10)\n",
    "        # same state after training\n",
    "        assert_array_almost_equal(som1._weights, som2._weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2f9d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec8b8ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
